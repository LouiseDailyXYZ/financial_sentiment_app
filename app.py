import streamlit as st
import requests
from bs4 import BeautifulSoup
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
from datetime import datetime, timedelta
import yfinance as yf

@st.cache_resource
def load_model():
    model = BertForSequenceClassification.from_pretrained("ProsusAI/finbert")
    tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

nlp = load_model()

st.set_page_config(page_title="è‚¡ç¥¨æ–°èæƒ…æ„Ÿåˆ†æå™¨", layout="centered")
st.title("ğŸ“ˆ ç¾è‚¡æ–°èæƒ…æ„Ÿåˆ†æå™¨")
st.markdown("è¼¸å…¥ç¾è‚¡ä»£ç¢¼ï¼ˆå¦‚ AAPLã€TSLAï¼‰ï¼Œç³»çµ±å°‡æ“·å–æœ€è¿‘ 14 æ—¥å…§çš„æ–°èä¸¦åˆ†ææƒ…æ„Ÿå‚¾å‘ã€‚")

ticker = st.text_input("è«‹è¼¸å…¥ç¾è‚¡ä»£ç¢¼ (å¦‚ AAPLã€TSLA)").upper()

if ticker:
    st.info("ğŸ“¡ æ­£åœ¨æœå°‹ç›¸é—œæ–°è...")
    news_links = search_news(ticker)

    sentiments = []
    summaries = []

    for link in news_links:
        text = extract_article_text(link)
        if text:
            short_text = text[:512]
            result = nlp(short_text)
            sentiments.append(result[0]['score'] * (1 if result[0]['label'] == 'positive' else -1 if result[0]['label'] == 'negative' else 0))
            summaries.append((link, result[0]['label'], round(result[0]['score'] * 100, 2)))

    if summaries:
        st.subheader("ğŸ“° æœ€æ–°æ–°èæƒ…ç·’åˆ†æçµæœï¼š")
        for i, (link, label, score) in enumerate(summaries, 1):
            st.markdown(f"**{i}. [{label}] ({score}%)** âœ [æŸ¥çœ‹æ–°è]({link})")

        avg_sentiment = sum(sentiments) / len(sentiments)
        st.subheader("ğŸ“Š æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ï¼š")
        if avg_sentiment > 0.1:
            st.success(f"åæ­£é¢ï¼š{round(avg_sentiment, 2)}")
        elif avg_sentiment < -0.1:
            st.error(f"åè² é¢ï¼š{round(avg_sentiment, 2)}")
        else:
            st.warning(f"æƒ…ç·’ä¸­ç«‹ï¼š{round(avg_sentiment, 2)}")
    else:
        st.warning("æœªæ“·å–åˆ°æœ‰æ•ˆæ–°èæˆ–æƒ…æ„Ÿç„¡æ³•åˆ†æã€‚")


# Bing News Search API (å…è²»æ–¹å¼ä»¥çˆ¬èŸ²ç‚ºä¸»)
import feedparser

def search_news(ticker, max_articles=10):
    today = datetime.today()
    after_date = (today - timedelta(days=14)).strftime("%Y-%m-%d")
    rss_url = f"https://news.google.com/rss/search?q={ticker}+stock+after:{after_date}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(rss_url)

def extract_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        # å„ªå…ˆæŠ“å– <article> æˆ– <div class="main-content">
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:
            main_div = soup.find('div', class_='main-content')
            paragraphs = main_div.find_all('p') if main_div else soup.find_all('p')

        text = '\n'.join([p.get_text() for p in paragraphs])
        if len(text) < 200:
            text = soup.get_text()
        return text.strip()
    except Exception:
        return None


    links = []
    for entry in feed.entries:
        st.write('ğŸ§¾ æ‰¾åˆ°æ–°èï¼š', entry.title, entry.link)
        links.append(entry.link)
        if len(links) >= max_articles:
            break

    return links