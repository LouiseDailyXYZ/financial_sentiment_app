import streamlit as st
import requests
from bs4 import BeautifulSoup
from transformers import BertTokenizer, BertForSequenceClassification, pipeline

@st.cache_resource
def load_model():
    model = BertForSequenceClassification.from_pretrained("ProsusAI/finbert")
    tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

def extract_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = '\n'.join([p.get_text() for p in paragraphs])
        return text.strip()
    except Exception:
        return None

nlp = load_model()

st.set_page_config(page_title="é‡‘èæƒ…æ„Ÿåˆ†æå™¨", layout="centered")
st.title("ğŸ“ˆ é‡‘èæ–‡ç« æƒ…æ„Ÿåˆ†æå™¨")
st.markdown("è¼¸å…¥ä¸€ç¯‡é‡‘èæ–°èé€£çµï¼Œè‡ªå‹•åˆ†æå…¶æƒ…ç·’æ˜¯æ­£é¢ã€è² é¢æˆ–ä¸­ç«‹ã€‚")

url = st.text_input("è«‹è¼¸å…¥æ–‡ç« é€£çµï¼ˆä¾‹å¦‚ Bloombergã€Reutersã€Yahoo Financeï¼‰")

if url:
    with st.spinner("â³ æ­£åœ¨æ“·å–æ–‡ç« å…§å®¹..."):
        text = extract_article_text(url)

    if not text:
        st.error("âŒ ç„¡æ³•æ“·å–å…§å®¹ï¼Œè«‹ç¢ºèªé€£çµæ˜¯å¦æ­£ç¢º")
    else:
        st.subheader("ğŸ“„ æ–‡ç« å…§å®¹")
        st.write(text[:1000] + ("..." if len(text) > 1000 else ""))

        st.subheader("ğŸ“Š æƒ…æ„Ÿåˆ†æçµæœ")
        result = nlp(text[:512])
        label = result[0]['label']
        score = round(result[0]['score'] * 100, 2)
        st.success(f"**{label}**ï¼ˆä¿¡å¿ƒå€¼ï¼š{score}%ï¼‰")
