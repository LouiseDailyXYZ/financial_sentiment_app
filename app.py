import streamlit as st
import requests
from bs4 import BeautifulSoup
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
from datetime import datetime, timedelta
import yfinance as yf
import feedparser

st.set_page_config(page_title="ËÇ°Á•®Êñ∞ËÅûÊÉÖÊÑüÂàÜÊûêÂô®", layout="centered")

@st.cache_resource
def load_model():
    model = BertForSequenceClassification.from_pretrained("ProsusAI/finbert")
    tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

nlp = load_model()

def search_news(ticker, max_articles=10):
    today = datetime.today()
    after_date = (today - timedelta(days=14)).strftime("%Y-%m-%d")
    rss_url = f"https://news.google.com/rss/search?q={ticker}+stock+after:{after_date}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(rss_url)

    links = []
    for entry in feed.entries:
        st.write('üìÑ ÊâæÂà∞Êñ∞ËÅûÔºö', entry.title)
        links.append(entry.link)
        if len(links) >= max_articles:
            break

    return links

def extract_article_text(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:
            main_div = soup.find('div', class_='main-content')
            paragraphs = main_div.find_all('p') if main_div else soup.find_all('p')
        text = '\n'.join([p.get_text() for p in paragraphs])
        if len(text) < 200:
            text = soup.get_text()
        return text.strip()
    except Exception:
        return None

st.title("üìà ÁæéËÇ°Êñ∞ËÅûÊÉÖÊÑüÂàÜÊûêÂô®")
st.markdown("Ëº∏ÂÖ•ÁæéËÇ°‰ª£Á¢ºÔºàÂ¶Ç AAPL„ÄÅTSLAÔºâÔºåÁ≥ªÁµ±Â∞áÊì∑ÂèñÊúÄËøë 14 Êó•ÂÖßÁöÑÊñ∞ËÅû‰∏¶ÂàÜÊûêÊÉÖÊÑüÂÇæÂêë„ÄÇ")

ticker = st.text_input("Ë´ãËº∏ÂÖ•ÁæéËÇ°‰ª£Á¢º (Â¶Ç AAPL„ÄÅTSLA)").upper()

if ticker:
    st.info("üì° Ê≠£Âú®ÊêúÂ∞ãÁõ∏ÈóúÊñ∞ËÅû...")
    news_links = search_news(ticker)

    sentiments = []
    summaries = []

    for link in news_links:
        text = extract_article_text(link)
        if text:
            short_text = text[:512]
            result = nlp(short_text)
            sentiments.append(result[0]['score'] * (1 if result[0]['label'] == 'positive' else -1 if result[0]['label'] == 'negative' else 0))
            summaries.append((link, result[0]['label'], round(result[0]['score'] * 100, 2)))

    if summaries:
        st.subheader("üì∞ ÊúÄÊñ∞Êñ∞ËÅûÊÉÖÁ∑íÂàÜÊûêÁµêÊûúÔºö")
        for i, (link, label, score) in enumerate(summaries, 1):
            st.markdown(f"**{i}. [{label}] ({score}%)** ‚ûú [Êü•ÁúãÊñ∞ËÅû]({link})")

        avg_sentiment = sum(sentiments) / len(sentiments)
        st.subheader("üìä Êï¥È´îÂπ≥ÂùáÊÉÖÁ∑íÂàÜÊï∏Ôºö")
        if avg_sentiment > 0.1:
            st.success(f"ÂÅèÊ≠£Èù¢Ôºö{round(avg_sentiment, 2)}")
        elif avg_sentiment < -0.1:
            st.error(f"ÂÅèË≤†Èù¢Ôºö{round(avg_sentiment, 2)}")
        else:
            st.warning(f"ÊÉÖÁ∑í‰∏≠Á´ãÔºö{round(avg_sentiment, 2)}")
    else:
        st.warning("Êú™Êì∑ÂèñÂà∞ÊúâÊïàÊñ∞ËÅûÊàñÊÉÖÊÑüÁÑ°Ê≥ïÂàÜÊûê„ÄÇ")